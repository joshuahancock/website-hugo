i <- i + 1
}
i <- 1
for (as.Date(day) in dates)){
temp <- subset(sessions.start, date == day)
count <- length(unique(temp$client_cognito_id))
daily.counts$date[i] <- date
daily.counts$users[i] <- count
i <- i + 1
}
for (as.Date(day) in dates){
temp <- subset(sessions.start, date == day)
count <- length(unique(temp$client_cognito_id))
daily.counts$date[i] <- date
daily.counts$users[i] <- count
i <- i + 1
}
class(dates)
dates.str <- as.character(dates)
i <- 1
for (day in dates.str){
temp <- subset(sessions.start, date == day)
count <- length(unique(temp$client_cognito_id))
daily.counts$date[i] <- date
daily.counts$users[i] <- count
i <- i + 1
}
sessions.start$date <- as.character(sessions.start$date)
sessions.start$date[1]
daily.counts <- data.frame(date = dates, users = 0)
daily.counts <- data.frame(date = dates, users = 0)
daily.counts$date <- as.character(daily.counts$date)
i <- 1
for (day in dates.str){
temp <- subset(sessions.start, date == day)
count <- length(unique(temp$client_cognito_id))
daily.counts$date[i] <- date
daily.counts$users[i] <- count
i <- i + 1
}
View(daily.counts)
dates.str
class(dat)
class(day)
day
daily.counts <- data.frame(date = dates, users = 0)
daily.counts$date <- as.character(daily.counts$date)
i <- 1
for (day in dates.str){
temp <- subset(sessions.start, date == day)
count <- length(unique(temp$client_cognito_id))
daily.counts$date[i] <- day
daily.counts$users[i] <- count
i <- i + 1
}
View(daily.counts)
as.Date(daily.counts$date[1])
daily.counts$date <- as.Date(daily.counts$date)
ggplot() + geom_line(data=post.counts, aes(x=date, y = freq)) +
geom_line(data=postcounts, aes(x=date,y=freq))
ggplot() + geom_line(data=post.counts, aes(x=date, y = freq)) +
geom_line(data=post.counts, aes(x=date,y=freq))
ggplot() + geom_line(data=post.counts, aes(x=date, y = freq)) + geom_line(data=post.counts, aes(x=date,y=freq))
ggplot() + geom_line(data=post.counts, aes(x=date, y = freq)) + geom_line(data=daily.counts, aes(x=date,y=freq))
ggplot() + geom_line(data=post.counts, aes(x=date, y = freq)) + geom_line(data=daily.counts, aes(x=date,y=users))
ggplotly()
ggplot() + geom_line(data=post.counts, aes(x=date, y = freq)) + geom_line(data=daily.counts, aes(x=date,y=users)) + geom_smooth()
ggplot() + geom_line(data=post.counts, aes(x=date, y = freq)) + geom_line(data=daily.counts, aes(x=date,y=users)) + xlim("2016-06-10", "2016-08-16")
ggplotly()
post.counts.filtered <- filter(post.counts, date >= "2016-06-10" & date < "2016-08-16")
daily.counts.filtered <- filter(daily.counts, date >= "2016-06-10" & date < "2016-08-16")
ggplot() + geom_line(data=post.counts.filtered, aes(x=date, y = freq)) + geom_line(data=daily.counts.filtered, aes(x=date,y=users))
ggplotly()
ggplot() + geom_line(data=post.counts.filtered, aes(x=date, y = freq, label = "Posts")) + geom_line(data=daily.counts.filtered, aes(x=date,y=users, label = "Users"))
ggplotly()
ggplot() + geom_line(data=post.counts.filtered, aes(x=date, y = freq, label = "Posts")) + geom_line(data=daily.counts.filtered, aes(x=date,y=users, color = "steelblue"))
ggplot() + geom_line(data=post.counts.filtered, aes(x=date, y = freq, label = "Posts")) + geom_line(data=daily.counts.filtered, aes(x=date,y=users, color = "Posts"))
ggplot() + geom_line(data=post.counts.filtered, aes(x=date, y = freq, color = "Users")) + geom_line(data=daily.counts.filtered, aes(x=date,y=users, color = "Posts"))
ggplotly()
ggplot() + geom_line(data=post.counts.filtered, aes(x=date, y = freq, color = "Users")) + geom_line(data=daily.counts.filtered, aes(x=date,y=users, color = "Posts")) + guides(color = FALSE)
ggplotly()
ggplot() + geom_line(data=post.counts.filtered, aes(x=date, y = freq, color = "Users")) + geom_line(data=daily.counts.filtered, aes(x=date,y=users, color = "Posts")) + guides(color = FALSE) + labs(x = "", y = "Count")
ggplot() + geom_line(data=post.counts.filtered, aes(x=date, y = freq, color = "Users")) + geom_line(data=daily.counts.filtered, aes(x=date,y=users, color = "Posts")) + guides(color = FALSE) + labs(x = "", y = "")
ggplotly()
ggplot() +  geom_line(data=daily.counts.filtered, aes(x=date,y=users)) + labs(x = "", y = "")
ggplotly()
ggplot() +  geom_line(data=daily.counts.filtered, aes(x=date,y=users)) + labs(x = "", y = "") + geom_smooth()
ggplot() + geom_line(data=daily.counts.filtered, aes(x=date,y=users)) + labs(x = "", y = "") + geom_smooth()
ggplot() + geom_line(data=daily.counts.filtered, aes(x=date,y=users)) + geom_smooth() + labs(x = "", y = "")
ggplot(daily.counts.filtered, aes(x=date, y=users)) + geom_line() + geom_smooth()
ggplotly()
myData <- left_join(daily.counts.filtered, post.counts.filtered)
View(myData)
myData$rate <- myData$freq / myData$users
ggplot(myData, aes(x=date, y=rate)) + geom_line() + geom_smooth()
ggplotly()
USER = "eventreader"
PASSWORD = "urXxzzb87tyRYHtKqp"
library(RPostgreSQL)
library(dplyr)
library(plyr)
postgresdriver <- dbDriver("PostgreSQL")
#http://docs.aws.amazon.com/redshift/latest/mgmt/connecting-from-psql.html
#https://blogs.aws.amazon.com/bigdata/post/Tx1G8828SPGX3PK/Connecting-R-with-Amazon-Redshift
#https://mobile.awsblog.com/post/Tx22DB1H8UNCDS9/Get-insights-into-your-mobile-app-usage-patterns-using-Amazon-Mobile-Analytics-a
#http://www.slideshare.net/AmazonWebServices/get-insights-into-your-app-usage-with-amazon-mobile-analytics-aws-mobile-web-day
redshift_connect <- dbConnect(postgresdriver,
host = "mobileanalyticsautoexporttoredshi-redshiftcluster-1y2nezarujs4v.c3njq5xys5sb.us-east-1.redshift.amazonaws.com",
port = "8192",
dbname = "events",
user = USER,
password = PASSWORD)
all.visits <- dbGetQuery(redshift_connect,
"SELECT
*
FROM
AWSMA.v_event
WHERE
event_type = 'visit'
")
# truncate lat and long for analysis
all.visits$lat.trunc <- substr(all.visits$a_latitude, 1, 6)
# one extra character in the substring to account for negative sign
all.visits$long.trunc <- substr(all.visits$a_longitude, 1, 7)
# count visits per user_id
visit.user.counts <- plyr::count(all.visits, "a_user_id")
# arrange by most visits to least
visit.user.counts <- dplyr::arrange(visit.user.counts,desc(freq))
visit.user.counts <- visit.user.counts[complete.cases(visit.user.counts), ]
id_vector <- unique(visit.user.counts$a_user_id)
for (i in 6:10){
id <- id_vector[i]
# create data frame for each user
temp.user <- dplyr::filter(all.visits, a_user_id == id)
temp.user.counts <- plyr::count(temp.user, c("lat.trunc", "long.trunc"))
temp.user.counts <- dplyr::arrange(temp.user.counts, desc(freq))
library(ggmap)
for (i in 1:4){
x.lat <- as.numeric(temp.user.counts$lat.trunc[i])
x.long <- as.numeric(temp.user.counts$long.trunc[i])
x.cords <- c(x.long, x.lat)
title <- paste("user_id:", id, ",", "visits:", temp.user.counts$freq[i])
temp.map <- get_map(location = x.cords, zoom = 18, maptype = "roadmap")
a <- ggmap(temp.map, extent = "device") +
geom_point(aes(x = as.numeric(a_longitude), y = as.numeric(a_latitude)), colour = "red", alpha = 0.1, size = 2, data = temp.user) +
ggtitle(title)
print(a)
invisible(readline(prompt="Press [enter] to continue"))
b <- ggmap(temp.map, extent = "device") +
#geom_density2d(data = temp.user,
#aes(x = as.numeric(a_longitude), y = as.numeric(a_latitude), size = 0.3)) +
stat_density2d(data = temp.user,
aes(x = as.numeric(a_longitude), y = as.numeric(a_latitude), fill = ..level.., alpha = ..level..), size = 0.01, bins = 16, geom = "polygon") +
scale_fill_gradient(low = "green", high = "red", guide = FALSE) +
scale_alpha(range = c(0, 0.5), guide = FALSE) +
ggtitle(title)
print(b)
invisible(readline(prompt="Press [enter] to continue"))
}
}
warnings)
warnings()
warnings()
warnings()
?t
?%*%
%*%
?apply
?exp
x = 3
exp(x)
pexp(6, 1/5, lower.tail = FALSE)
pexp(6, 1/5, lower.tail = TRUE)
library(readr)
hr_data <- read_csv("~/Projects/XPO Application Presentation/HR_comma_sep.csv")
View(hr_data)
library(readr)
sales <- read_csv("~/Projects/Slalom Case/HourlySales.csv",
col_types = cols(DateStringYYYYMMDD = col_character(),
Fiscal_Qtr = col_character(), Fiscal_dayofWk = col_character(),
Hour = col_character(), Store_ID = col_character()))
View(sales)
library(lubridate)
sales$date_formatted <- ymd(sales$DateStringYYYYMMDD)
sales <- sales[, -sales$DateStringYYYYMMDD]
sales <- sales[, -3]
store_ids <- unique(sales$Store_ID)
sales$week <- week(sales$date_formatted)
sales$Store_ID <- factor(sales$Store_ID)
sales$Fiscal_Qtr <- factor(sales$Fiscal_Qtr)
sales$Fiscal_Qtr <- factor(sales$Fiscal_dayofWk)
library(readr)
sales <- read_csv("~/Projects/Slalom Case/HourlySales.csv",
col_types = cols(DateStringYYYYMMDD = col_character(),
Fiscal_Qtr = col_character(), Fiscal_dayofWk = col_character(),
Hour = col_character(), Store_ID = col_character()))
View(sales)
sales$date_formatted <- ymd(sales$DateStringYYYYMMDD)
sales <- sales[, -3]
store_ids <- unique(sales$Store_ID)
sales$week <- week(sales$date_formatted)
sales$month <- month(sales$date_formatted)
model <- lm(data = sales, SalesRevenue ~ Hour, HourlyWeather, Fiscal_Qtr, Fiscal_dayofWk)
model <- lm(data = sales, SalesRevenue ~ Hour, HourlyWeather, Fiscal_Qtr)
model <- lm(data = sales, SalesRevenue ~ Hour)
summary(lm)
summary(model)
model <- lm(data = sales, SalesRevenue ~ daypart)
model <- lm(data = sales, SalesRevenue ~ Daypart)
summary(model)
model <- lm(data = sales, SalesRevenue ~ Daypart, Fiscal_Qtr)
summary(model)
sales$Daypart <- factor(sales$Daypart)
model <- lm(data = sales, SalesRevenue ~ Daypart, Fiscal_Qtr)
summary(model)
model <- lm(data = sales, SalesRevenue ~ Fiscal_Qtr)
summary(model)
model <- lm(data = sales, SalesRevenue ~ AvgHourlyTemp)
summary(model)
model <- lm(data = sales, SalesRevenue ~ AvgHourlyTemp, Daypart)
summary(model)
model <- lm(data = sales, SalesRevenue ~ AvgHourlyTemp, Daypart, HourlyWeather)
sales$HourlyWeather <- factor(sales$HourlyWeather)
model <- lm(data = sales, SalesRevenue ~ AvgHourlyTemp, Daypart, HourlyWeather)
model <- lm(data = sales, SalesRevenue ~ HourlyWeather)
summary(model)
library(randomForest)
randomForest(SalesRevenue ~ HourlyWeather, Hour, week, AvgHourlyTemp, data = sales)
attach(sales)
randomForest(SalesRevenue ~ HourlyWeather, Hour, week, AvgHourlyTemp, data = sales)
sales$HourlyWeather <- factor(sales$HourlyWeather)
typeof(sales$HourlyWeather)
typeof(sales$Store_ID)
sales$Store_ID <- factor(sales$Store_ID)
typeof(sales$Store_ID)
randomForest(SalesRevenue ~ HourlyWeather, Hour, week, AvgHourlyTemp, data = sales)
rf.df <- sales[, c(2, 3, 4, 5, 6, 7, 8, 11, 12)]
View(rf.df)
smp_size <- floor(0.75 * nrow(sales))
set.seed(123)
train_ind <- sample(seq_len(nrow(sales)), size = smp_size)
train <- rf.df[train_ind, ]
rf.df$Fiscal_Qtr <- factor(rf.df$Fiscal_Qtr)
rf.df$Fiscal_dayofWk <- factor(rf.df$Fiscal_dayofWk)
rf.df$Daypart <- factor(rf.df$Daypart)
rf.df$HourlyWeather <- factor(rf.df$HourlyWeather)
rf.df$Hour <- factor(rf.df$Hour)
rf.df$week <- factor(rf.df$week)
rf.df$month <- factor(rf.df$month)
rf.model <- randomForest(SalesRevenue ~., data = rf.df)
test <- rf.df[-train_ind, ]
(r2 <- rSquared(test$SalesRevenue, test$SalesRevenue - predict(model, test)))
library(miscTools)
(r2 <- rSquared(test$SalesRevenue, test$SalesRevenue - predict(model, test)))
(mse <- mean((test$SalesRevenue - predict(model, test))^2))
online_date <- sales %>% group_by(Store_ID) %>% summarise(min_year = min(date_formatted))
n = seq(1, 100,1)
boxplot(n)
sd(n)
library(rjags)
install.packages('rjags')
library(rjags)
install.packages('baseballr')
install.packages("devtools")
devtools::install_github("BillPetti/baseballr")
require(readr)
require(dplyr)
require(xml2)
require(magrittr)
require(baseballr)
# import a csv of indiidual dates used to build queries
dates_reduced <- read_csv("https://raw.githubusercontent.com/BillPetti/baseball_research_notebook/master/dates_statcast_build.csv")
# put dates into years
x2008season <- dates_reduced %>%
filter(substr(GameDate, 1, 4) == 2008)
x2009season <- dates_reduced %>%
filter(substr(GameDate, 1, 4) == 2009)
x2010season <- dates_reduced %>%
filter(substr(GameDate, 1, 4) == 2010)
x2011season <- dates_reduced %>%
filter(substr(GameDate, 1, 4) == 2011)
x2012season <- dates_reduced %>%
filter(substr(GameDate, 1, 4) == 2012)
x2013season <- dates_reduced %>%
filter(substr(GameDate, 1, 4) == 2013)
x2014season <- dates_reduced %>%
filter(substr(GameDate, 1, 4) == 2014)
x2015season <- dates_reduced %>%
filter(substr(GameDate, 1, 4) == 2015)
x2016season <- dates_reduced %>%
filter(substr(GameDate, 1, 4) == 2016)
x2017season <- dates_reduced %>%
filter(substr(GameDate, 1, 4) == 2017)
View(dates_reduced)
update.packages('R6')
library(dplyr)
library(blogdown)
blogdown::serve_site()
blogdown::build_site()
getwd()
my_wd <- "C:/Users/Josh/Documents/GitHub/"
setwd(my_wd)
blogdown::build_site()
my_wd <- "C:/Users/Josh/Documents/GitHub/website-hugo"
setwd(my_wd)
blogdown::build_site()
blogdown::build_site()
blogdown::build_site()
library("mvtnorm")
library("data.table")
# Reading data and convert to data table
setwd("C:/Users/Josh/Documents/GitHub/joshuahancock.github.io/data_sets/")
data <- fread("C:/Users/Josh/Documents/GitHub/joshuahancock.github.io/data_sets/semeion.csv", header = FALSE)
x <- data[, 1:256]
labels <-apply(data[, 257:266], 1, function(xx){return(which(xx == "1") -1)})
# k is the number of clusters
k <- 10
# n is the number of observations
n <- nrow(x)
# d is the number of diwmensions
d <- ncol(x)
# q represents the number of principal components and will need to be manually changed
q <- 6
# x.clusters are the clusters using k means and 100 random starts
x.clusters <- kmeans(x, k, nstart = 100)
# n by k matrix, initialized with zeros
gamma <- matrix(0, n, k)
# indicate the inital custer membership with a binary label
for(i in 1:n) {gamma[i, x.clusters$cluster[i]] = 1}
# the number of members in each cluster
N <- colSums(gamma)
# the percentage of the data set in cluster k
pi.hat <- N/n
# the mean for each pixel in each cluster
# note: a matrix is required for the t() function
mu.hat <- (t(gamma) %*% data.matrix(x))/ N
sigma.k <- function(GAMMA, X, MU, X.CLUSTERS, q){
# initialize empty list to hold sigma matricies
sigma.list <- list()
# normailze columns
g.norm <- t(t(GAMMA)/N)
# iterate over each cluster
for(i in 1:k){
# for each observation in the data set, subtract the mu
x.mu <- t(apply(X, 1, function(xx) xx-MU[i, ]))
# create convariance matrix
g.x.mu <- t(g.norm[, i] * x.mu) %*% x.mu
# eigen decomposition
x.decomp <- eigen(g.x.mu)
# principal components step
if(q > 0){
sig.hat <- sum(x.decomp$values[as.integer(q + 1):as.integer(d)]) / (d - q)
decomp.vec <- x.decomp$vectors[, 1:q]
lambda <- x.decomp$values[1:q]
W <- decomp.vec %*% (diag(q) * sqrt(lambda - sig.hat)) %*% t(decomp.vec)
sigma.list[[i]] <- W %*% t(W) + sig.hat * diag(d)
}
else {
sigma.list[[i]] <- diag(d) * sum(x.decomp$values) / d
}
}
return(sigma.list)
}
sigma.hat <- sigma.k(gamma, x, muHat, x.clusters, q)
sigma.hat <- sigma.k(gamma, x, mu.hat, x.clusters, q)
update.gamma <- function(PI, SIGMA , X, MU){
# a temp matrix to store gamma values
p.mu.sig <- matrix(nrow = n, ncol = k)
# iterate over clusters
for(i in 1:k){
# weight raw probabilities by pi
p.mu.sig[, i] <- PI[i] * dmvnorm(X, MU[i, ], SIGMA[[i]])
}
weights <- rowSums(p.mu.sig)
# returns normalized rows
return(p.mu.sig / weights)
}
LL <- function(PI, SIGMA, X, MU){
# keeps a running track of the sum
iter.sum <- c(rep(0, n))
for(i in 1:k){
iter.sum <- iter.sum + PI[i] * dmvnorm(X, MU[i, ], SIGMA[[i]])
}
return(sum(log(iter.sum)))
}
AIC <- function(LL, q){
return(-2 * LL + 2 * (d * q + 1 - q* (q - 1) / 2))
}
LL.list <- c()
AIC.list <- c()
iters <- 0
while(iters < 51){
gamma <- update.gamma(piHat, sigma.hat, x , mu.hat)
N <- colSums(gamma)
pi.hat <- N / n
mu.hat <- (t(gamma) %*% as.matrix(x)) / N
sigma.hat <- sigma.k(gamma, x, mu.hat, x, q)
log.likelihood <- LL(pi.hat, sigma.hat, x, mu.hat)
LL.list <- c(LL.list, log.likelihood)
iters <- iters + 1
}
LL.list <- c()
AIC.list <- c()
iters <- 0
while(iters < 51){
gamma <- update.gamma(pi.hat, sigma.hat, x , mu.hat)
N <- colSums(gamma)
pi.hat <- N / n
mu.hat <- (t(gamma) %*% as.matrix(x)) / N
sigma.hat <- sigma.k(gamma, x, mu.hat, x, q)
log.likelihood <- LL(pi.hat, sigma.hat, x, mu.hat)
LL.list <- c(LL.list, log.likelihood)
iters <- iters + 1
}
AIC.q <- AIC(tail(LL.list, 1), q)
AIC.list <- c(AIC.list, AIC.q)
if(q == 0){
ll.plot <- as.matrix(LL.list)
} else{
ll.plot <- cbind(ll.plot, LL.list)
}
LL.list <- c()
AIC.list <- c()
iters <- 0
while(iters < 51){
gamma <- update.gamma(pi.hat, sigma.hat, x , mu.hat)
N <- colSums(gamma)
pi.hat <- N / n
mu.hat <- (t(gamma) %*% as.matrix(x)) / N
sigma.hat <- sigma.k(gamma, x, mu.hat, x, q)
log.likelihood <- LL(pi.hat, sigma.hat, x, mu.hat)
LL.list <- c(LL.list, log.likelihood)
iters <- iters + 1
}
AIC.q <- AIC(tail(LL.list, 1), q)
AIC.list <- c(AIC.list, AIC.q)
if(iters == 0){
ll.plot <- as.matrix(LL.list)
} else{
ll.plot <- cbind(ll.plot, LL.list)
}
LL.list <- c()
AIC.list <- c()
iters <- 0
while(iters < 51){
gamma <- update.gamma(pi.hat, sigma.hat, x , mu.hat)
N <- colSums(gamma)
pi.hat <- N / n
mu.hat <- (t(gamma) %*% as.matrix(x)) / N
sigma.hat <- sigma.k(gamma, x, mu.hat, x, q)
log.likelihood <- LL(pi.hat, sigma.hat, x, mu.hat)
LL.list <- c(LL.list, log.likelihood)
iters <- iters + 1
}
AIC.q <- AIC(tail(LL.list, 1), q)
AIC.list <- c(AIC.list, AIC.q)
if(iters == ){
LL.list <- c()
AIC.list <- c()
iters <- 0
while(iters < 51){
gamma <- update.gamma(pi.hat, sigma.hat, x , mu.hat)
N <- colSums(gamma)
pi.hat <- N / n
mu.hat <- (t(gamma) %*% as.matrix(x)) / N
sigma.hat <- sigma.k(gamma, x, mu.hat, x, q)
log.likelihood <- LL(pi.hat, sigma.hat, x, mu.hat)
LL.list <- c(LL.list, log.likelihood)
iters <- iters + 1
}
AIC.q <- AIC(tail(LL.list, 1), q)
AIC.list <- c(AIC.list, AIC.q)
if(iters == ){
LL.list <- c()
AIC.list <- c()
iters <- 0
while(iters < 51){
gamma <- update.gamma(pi.hat, sigma.hat, x , mu.hat)
N <- colSums(gamma)
pi.hat <- N / n
mu.hat <- (t(gamma) %*% as.matrix(x)) / N
sigma.hat <- sigma.k(gamma, x, mu.hat, x, q)
log.likelihood <- LL(pi.hat, sigma.hat, x, mu.hat)
LL.list <- c(LL.list, log.likelihood)
iters <- iters + 1
}
AIC.q <- AIC(tail(LL.list, 1), q)
AIC.list <- c(AIC.list, AIC.q)
if(iters == 1){
ll.plot <- as.matrix(LL.list)
} else{
ll.plot <- cbind(ll.plot, LL.list)
}
print(iters)
LL.list <- c()
AIC.list <- c()
iters <- 0
while(iters < 51){
gamma <- update.gamma(pi.hat, sigma.hat, x , mu.hat)
N <- colSums(gamma)
pi.hat <- N / n
mu.hat <- (t(gamma) %*% as.matrix(x)) / N
sigma.hat <- sigma.k(gamma, x, mu.hat, x, q)
log.likelihood <- LL(pi.hat, sigma.hat, x, mu.hat)
LL.list <- c(LL.list, log.likelihood)
iters <- iters + 1
}
blogdown::build_site()
getwd()
blogdown::build_site()
blogdown::serve
blogdown::serve_site()
