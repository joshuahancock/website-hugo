---
title: "Using Regression Analysis to Predict MLB Attendance"
author: "Josh Hancock"
date: '2017-06-02'
output: pdf_document
coverImage: heat_plot_large.jpeg
tags:
- linear regression
- R
thumbnailImage: images/heat_plot_large.jpeg
categories: regression
---

The standard MLB season is 162 games long and each team typically plays 81 games at home and 81 games in the stadiums of the opposing teams. Using data from historical MLB seasons, we can build a model to predict the total attendance at the 81 home games for each team. In many business applications, it's of particualr interest to stakeholders to understand which factors influence the revenue-creating side of the business. For this reason, we'll choose to use a linear regression model, which may fall short of deep-learning models when it comes to predictive power, but will provide an output that is interpritable and may allow for a better understaning of the business as a whole.

First, we'll obtain and clean data. Attendance can be influenced by many factors beyond the playing field, so it will be important to include data on both team performance factors and the demographics of the city and fan base for each team. For team data, we can use [Baseball Reference](www.baseballreference.com) to grab results between 2006 and 2014 for each of the 30 teams, giving us 270 observations. In addition to the win/loss records of each team,  we have home attendance, wins, payroll, stadium name, stadium capacity, stadium age, number of years a team has been in a city, playoff appearances for each team, the number of all-stars for each team, the number of home-runs hit by each team, and the number of professional sports teams in in each city for each observation. 

Using publically available demographic data for each city, we can also include the number of people, the number of households, and the median income for various drive times (15, 30, 45, and 60 minutes) from each stadium. We then imported the data into R and inspected it for any obvious problems (graphically and using the *summary* command).

```{r, include=FALSE}
require(faraway)
require(car)
require(leaps)
require(ggplot2)
```

```{r}
mlbattendance_final = read.csv("mlbattendance_final.csv", header = TRUE)
attach(mlbattendance_final)
summary(mlbattendance_final)
```

 reserve 10% of our data for testing purposes before starting our analysis.

```{r,echo=FALSE}
testdata <- mlbattendance_final[(seq(from=10, to=nrow(mlbattendance_final), by=10)),]
traindata <- mlbattendance_final[-(seq(from=10, to=nrow(mlbattendance_final), by=10)),]
```

```{r}
nrow(testdata)
nrow(traindata)
```

We begin our analysis with 243 observations x 27 variables (including observation names) for the training data set.We started our analysis with a base model:

```{r}
basemod <- lm(nextAttend ~ currentAttend + currentW + priorW + X.capacity + payrollM + 
                stadiumCap + stadiumAge + yearsInCity + playoffsBin + proTeams + 
                allstars + hrs + pop15 + pop30 + pop45 + pop60 + households15 + 
                households30 + households45 + households60 + medInc15 + medInc30 + medInc45 +
                medInc60,data=traindata)
```

Note: See Appendix A for an explanation of the data and variable names


Initially, we suspected a strong correlation between many variables in our data set. We started by looking at the correlation matrix (See Appendix B).

There were many strong correlations, especially with the demographic data. We decided to build a different base model for each level of drive time (15,30,45,60) data to determine which one has the most significance in the current model:

```{r}
lmod15<-lm(nextAttend ~ currentAttend + currentW + priorW + X.capacity + payrollM + 
             stadiumCap + stadiumAge + yearsInCity + playoffsBin + proTeams + allstars + 
             hrs + pop15 + households15 + medInc15,data=traindata)


lmod30<-lm(nextAttend ~ currentAttend + currentW + priorW + X.capacity + payrollM + 
             stadiumCap + stadiumAge + yearsInCity + playoffsBin + proTeams + allstars + 
             hrs + pop30 + households30 + medInc30,data=traindata)


lmod45<-lm(nextAttend ~ currentAttend + currentW + priorW + X.capacity + payrollM + 
             stadiumCap + stadiumAge + yearsInCity + playoffsBin + proTeams + allstars + 
             hrs + pop45 + households45 + medInc45,data=traindata)


lmod60<-lm(nextAttend ~ currentAttend + currentW + priorW + X.capacity + payrollM + 
             stadiumCap + stadiumAge + yearsInCity + playoffsBin + proTeams + allstars + 
             hrs + pop60 + households60 + medInc60,data=traindata)

```

All models were similar in adjusted $r^2$, so we selected the 60-minute model, which seemed to have the most significance in the individual drive-time variables. Even after selecting a single level of demographic data, there still seemed to be issues with correlated predictors, so we decided to view the variance inflation factor(VIF) for the 60-minute model:

```{r,include=FALSE}
x2 <- model.matrix(lmod60)[,-1]
round(cor(x2[,1:15]),2)
```

```{r}
vif(lmod60)
```

There seems to be two issues that need to be addressed. There is a very large VIF for currentAttend, X.capacity, pop60, and households60. stadiumCap also has a large VIF, but we will choose to address that after addressing the higher values. We start by removing X.capacity and households60.

```{r}
lmod <- lm(nextAttend ~ currentAttend + currentW + priorW + payrollM + stadiumCap + stadiumAge + yearsInCity + playoffsBin + proTeams + allstars + hrs + pop60 + medInc60,data=traindata)
vif(lmod)
```

All the VIF levels are under 10 (including *stadiumCap*), so we now move to graphically checking the variance (see Appendix C for plot).

From the plot we can safely assess that the variance appears to be constant and no further investigation is needed.
Next, we will check normality assumptions (see Appendix C for plot). 

The qqplot appears to show that the data is short-tailed, which is acceptable. We can conclude that no transformation of our model is needed because there doesn't appear to be any problems with variance or linearity.

Next we will check for high leverages in our data:

```{r}
hatv <- hatvalues(lmod)
threshold <- (2*14)/243
hatv_true <- hatv>threshold
which(hatv_true, useNames = TRUE)
```

There are a few observations that have a higher leverage than the $\textit{2p/n}$ ratio of 0.1037 (see Appendix C for plot). At this point we decide that they were not severe enough to immediately remove and should be assessed in presence of other tests. Next, we looked for outliers using the Bonferoni Correction:

```{r}
stud <- rstudent(lmod)
```

This gives us studentized residuals. Now, calculate the Bonferoni critical value:

```{r}
bonf <- qt((0.05/243*2),232)
abs_bonf <- abs(bonf)
abs_stud <- abs(stud)
bonf_points <- abs_stud > abs_bonf
abs_bonf
```

As we an see, there is one observation that exceeds the Bonferoni critical value. Because the data set is fairly large (n=243), we are not overly concerned with outliers. In order to test for influential points, we calculated the Cooks Distance for the data.  

```{r}
cook <- cooks.distance(lmod)
```

We then plotted the half normal plot of Cooks Distance and use the $4/(n-p-1)$ rule of thumb to check for any influential points in the data (see Appendix C for plot). From this we identified 12 points that appear to be influential points. We then removed those points and moved on with our diagnostics.

```{r}
newtrain <- subset(traindata, cook < 0.01754)
```

```{r, include=FALSE}
lmod <- lm(nextAttend ~ currentAttend + currentW + priorW + payrollM + stadiumCap + stadiumAge + yearsInCity + playoffsBin + proTeams + allstars + hrs + pop60 + medInc60,data=newtrain)
lmod2 <- lm(nextAttend ~ currentW + priorW + payrollM + stadiumCap + stadiumAge + yearsInCity + playoffsBin + proTeams + allstars + hrs + pop60 + medInc60, data=traindata)
```

Confident that our data and model assumptions are sound, we chose to move on to the shrinkage phase of the diagnostics. 

```{r}
b <- regsubsets((nextAttend ~ currentAttend + currentW + priorW + payrollM + stadiumCap + stadiumAge + yearsInCity + playoffsBin + proTeams + allstars + hrs + pop60 + medInc60), data=newtrain, nvmax=14)
rs <- summary(b)
```

Note: See Appendix E for predictor logic matrix

```{r}
AIC <- 231*log(rs$rss/231) + (2:14)*2
which.min(AIC)
```

*AIC* suggests the nine predictor model (see Appendix C for plot). Next, we'll look at the *Mallows CP* criterion (see Appendix C for plot). *Mallows CP* suggests a model with ten predictors. Next, we looked at *adjusted $r^2$*. 

```{r}
which.max(rs$adjr2)
```

*Adjusted $r^2$* suggests 10 predictors. As we did further analysis to decide on a final model, something became clear. The model that we were considering had one predictor that was much more significant than the others: *currentAttend*. Including this predictor in our model gave us a higher degree of accuracy, which is desirable in a prediction model. However, this predictor appeared to already contain much of the information we sought to include in our model by using additional variables and would possibly limit the amount of inference that could be achieved compared to a model that uses more predictors. We decided to branch our model into two different versions: one with *currentAttend* and one without. We checked the Cooks Distances for this model with the original training data set and found that slightly fewer points seemed to be influential, so we made a separate subset for the second model.

```{r}
cook2 <- cooks.distance(lmod2)
newtrain2 <- subset(traindata, cook2 < 0.01746)
```

Here is the second version of the model:
```{r}
lmod2 <- lm(nextAttend ~ currentW + priorW + payrollM + stadiumCap + stadiumAge + yearsInCity + playoffsBin + proTeams + allstars + hrs + pop60 + medInc60, data=newtrain2)
```

After running the same diagnostics on the second model as we did on the original, *AIC* suggested ten predictors, *CP* suggested 11 predictors, and *adjusted $r^2$* suggested 11. 

After considering the suggested number of predictors from each criterion, we removed predictors using the logic matrix and came up with the following models:

```{r}
final_lmod <- lm(nextAttend ~ currentAttend + currentW + priorW + payrollM + stadiumCap + stadiumAge + yearsInCity + proTeams,data=newtrain)

final_lmod2 <- lm(nextAttend ~ hrs + stadiumAge + pop60 + priorW + playoffsBin + medInc60 + payrollM + currentW + yearsInCity + stadiumCap, data = newtrain2)


```

Taking a look at the coefficients, we decided to scale *stadiumCap* to the same units as the attendance numbers.

```{r}

final_lmod <- lm(nextAttend ~ currentAttend + currentW + priorW + payrollM + I(stadiumCap/1000) + stadiumAge + yearsInCity + proTeams,data=newtrain)

final_lmod2 <- lm(nextAttend ~ hrs + stadiumAge + pop60 + priorW + playoffsBin + medInc60 + payrollM + currentW + yearsInCity + I(stadiumCap), data = newtrain2)

sumary(final_lmod)
sumary(final_lmod2)
```

For an interpretation of the coefficients, we start with the model containing *currentAttend*:

**intercept**: no meaningful interpretation (not possible to have negative attendance)  
**currentAttend**: for every 1000 people that attend in the current year, 926 people can be expected to attend next year (ceteris paribus)  
**currentW**: for each additional game a team wins, we can expect and additional 7780 people to attend the next year (ceteris paribus)  
**priorW**: for each game a team won last season, the expected attendance will drop by 3459 people in two seasons(ceteris paribus). This is counterintuitive and may be due to a correction effect resulting from other predictors  
**payrollM**: for each additional million dollars a team spends on payroll, the attendance of the next season can be expected to drop by 2189 (ceteris paribus). This is also counterintuitive and could also be a correcting effect.  
**stadiumCap**: for each additional 1000 seats in capacity, the attendance can be expected to increase by 9281 people over the course of a season(ceteris paribus)  
**stadiumAge**: for each additional year in stadium age, the attendance can be expected to increase by 1007(ceteris paribus)  
**yearsInCity**: for each additional year a franchise has been located in its current city, we can expect an additional 705 people to attend (ceteris paribus)  
**proTeams**: for every additional professional team in the metro area, attendance will increase by 13101 per season (ceteris paribus) 


There are a few differences in the coefficients between the two models. Most notably, all of the coefficients became positive (except for the intercept and *hrs*) in the second model. This leads us to believe that the *currentAttend* data was a very powerful vacuum, so to speak, and all the information that is sucked up by and contained inside of it needs to be corrected by other covariates contained in the model. In the absence of *currentAttend*, many of the other predictors' significance levels increased as they absorbed some of the significance abandoned by *currentAttend*. Additionally, *hrs*, *pop60*, and *medInc60* are included in the model and are significant at the 0.15, 0.10, and 0.01 levels, respectively. 

We decided to use both models to fit values to the test data. First, we define a function that calculates *rmse*:

```{r}
rmse <- function(x,y)sqrt(mean((x-y)^2))
```

Now we will compare the two models.

The model with *currentAttend*:

```{r}
rmse(fitted(final_lmod),newtrain$nextAttend)
rmse(predict(final_lmod,testdata),testdata$nextAttend)
```

The model without *currentAttend*:

```{r}
rmse(fitted(final_lmod2),newtrain2$nextAttend)
rmse(predict(final_lmod2,testdata),testdata$nextAttend)
```

As we expected, the model that includes *currentAttend* has the lower *rmse* value for the train and test cases.

Graphically:

```{r,echo=FALSE, message=FALSE,fig.height=4,fig.width=6}
attach(testdata)
testvals <- predict(final_lmod,testdata)
plot(currentAttend,testvals, ylab = "Predicted Values for Test Data", xlab="Actual Values from Test Data", main = "Model With currentAttend")
abline(0,1)
testvals <- predict(final_lmod2,testdata)
plot(currentAttend,testvals, ylab = "Predicted Values for Test Data", xlab="Actual Values from Test Data", main = "Model without currentAttend")
abline(0,1)
```
